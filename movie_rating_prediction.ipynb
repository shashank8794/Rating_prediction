{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "rating = keras.datasets.imdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "(train_data, train_label),(test_data, test_label) = rating.load_data(num_words=10000)#num_words=10000 means it will load only words which are frequently used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 14, 22, 16, 43, 530, 973, 1622, 1385, 65, 458, 4468, 66, 3941, 4, 173, 36, 256, 5, 25, 100, 43, 838, 112, 50, 670, 2, 9, 35, 480, 284, 5, 150, 4, 172, 112, 167, 2, 336, 385, 39, 4, 172, 4536, 1111, 17, 546, 38, 13, 447, 4, 192, 50, 16, 6, 147, 2025, 19, 14, 22, 4, 1920, 4613, 469, 4, 22, 71, 87, 12, 16, 43, 530, 38, 76, 15, 13, 1247, 4, 22, 17, 515, 17, 12, 16, 626, 18, 2, 5, 62, 386, 12, 8, 316, 8, 106, 5, 4, 2223, 5244, 16, 480, 66, 3785, 33, 4, 130, 12, 16, 38, 619, 5, 25, 124, 51, 36, 135, 48, 25, 1415, 33, 6, 22, 12, 215, 28, 77, 52, 5, 14, 407, 16, 82, 2, 8, 4, 107, 117, 5952, 15, 256, 4, 2, 7, 3766, 5, 723, 36, 71, 43, 530, 476, 26, 400, 317, 46, 7, 4, 2, 1029, 13, 104, 88, 4, 381, 15, 297, 98, 32, 2071, 56, 26, 141, 6, 194, 7486, 18, 4, 226, 22, 21, 134, 476, 26, 480, 5, 144, 30, 5535, 18, 51, 36, 28, 224, 92, 25, 104, 4, 226, 65, 16, 38, 1334, 88, 12, 16, 283, 5, 16, 4472, 113, 103, 32, 15, 16, 5345, 19, 178, 32]\n"
     ]
    }
   ],
   "source": [
    "print(train_data[0]) # rating is in digital(value). we need to change it to user friendly language(key-normal sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to convert digital language into user friendly language\n",
    "word_index = rating.get_word_index()\n",
    "word_index = {k:(v+4) for k, v in word_index.items()}   # v+4 because we are adding more keys. so rest every keys will shift by 4\n",
    "word_index[\"<PAD>\"]=0\n",
    "word_index[\"<START>\"]=1\n",
    "word_index[\"<UNK>\"]=2\n",
    "word_index[\"<UNUSED>\"]=3\n",
    "word_index[\"<BR>\"]=4\n",
    "reverse_word_index = dict([(value,key) for (key, value) in word_index.items()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = keras.preprocessing.sequence.pad_sequences(train_data, value=word_index[\"<PAD>\"], padding=\"post\", maxlen=500)\n",
    "test_data = keras.preprocessing.sequence.pad_sequences(test_data, value=word_index[\"<PAD>\"], padding=\"post\", maxlen=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_review(text):\n",
    "    return \" \".join([reverse_word_index.get(i, \"?\") for i in text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<START> i but comics and every a suggested great in scenario film drawn the films ask <BR> life to when cut the seen a <BR> why fact looks film <BR> few trying br seats <UNK> <BR> slash arms films allen movie <BR> but occurred of and considered likely is is <BR> think potential by third <UNK> for japanese party the streets <UNK> about when almost this mostly choppy etc i decides <BR> perfect the 14 a are people two end <BR> america's choppy bare perfect to overlooked man the some it after there story actually i but actors to <BR> viewer might at slash arms about human be a like definite think movie time american in who towards the ballet this about david writing complicated ever and what year was <BR> brainless pretty still psychological choppy andy writing views out and understand same <BR> <UNK> formulaic trying united <BR> confusing a <BR> only when me br either i with to and joke favor a literary japanese <UNK> the instant he perform <UNK> very <BR> third the break basically of <BR> allowed getting music is is it raw mostly choppy i to be a are seen two this remembered for ruled held you've slow i but to considered movie about priceless the about day of <UNK> and writer sci slightest so <BR> won't has not down of really by superior but if since she these respect i to three <BR> no is is turn about when became of look br awful repeating not give his of means in know <BR> 2000 <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n"
     ]
    }
   ],
   "source": [
    "print(decode_review(test_data[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500 500\n"
     ]
    }
   ],
   "source": [
    "print(len(test_data[0]), len(test_data[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\shash\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow_core\\python\\keras\\initializers.py:119: calling RandomUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From C:\\Users\\shash\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow_core\\python\\ops\\resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "WARNING:tensorflow:From C:\\Users\\shash\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow_core\\python\\ops\\nn_impl.py:183: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "Train on 15000 samples, validate on 10000 samples\n",
      "Epoch 1/40\n",
      "15000/15000 [==============================] - 5s 357us/sample - loss: 0.6928 - acc: 0.5035 - val_loss: 0.6926 - val_acc: 0.4991\n",
      "Epoch 2/40\n",
      "15000/15000 [==============================] - 3s 231us/sample - loss: 0.6915 - acc: 0.5300 - val_loss: 0.6906 - val_acc: 0.5167\n",
      "Epoch 3/40\n",
      "15000/15000 [==============================] - 3s 231us/sample - loss: 0.6880 - acc: 0.5575 - val_loss: 0.6855 - val_acc: 0.6060\n",
      "Epoch 4/40\n",
      "15000/15000 [==============================] - 3s 225us/sample - loss: 0.6803 - acc: 0.6135 - val_loss: 0.6755 - val_acc: 0.7554\n",
      "Epoch 5/40\n",
      "15000/15000 [==============================] - 5s 327us/sample - loss: 0.6659 - acc: 0.7177 - val_loss: 0.6586 - val_acc: 0.7505\n",
      "Epoch 6/40\n",
      "15000/15000 [==============================] - 3s 223us/sample - loss: 0.6432 - acc: 0.7715 - val_loss: 0.6349 - val_acc: 0.7179\n",
      "Epoch 7/40\n",
      "15000/15000 [==============================] - 3s 223us/sample - loss: 0.6123 - acc: 0.7803 - val_loss: 0.6017 - val_acc: 0.7855\n",
      "Epoch 8/40\n",
      "15000/15000 [==============================] - 3s 222us/sample - loss: 0.5755 - acc: 0.8093 - val_loss: 0.5657 - val_acc: 0.8045\n",
      "Epoch 9/40\n",
      "15000/15000 [==============================] - 5s 340us/sample - loss: 0.5348 - acc: 0.8270 - val_loss: 0.5277 - val_acc: 0.8199\n",
      "Epoch 10/40\n",
      "15000/15000 [==============================] - 4s 239us/sample - loss: 0.4933 - acc: 0.8449 - val_loss: 0.4915 - val_acc: 0.8272\n",
      "Epoch 11/40\n",
      "15000/15000 [==============================] - 3s 222us/sample - loss: 0.4541 - acc: 0.8571 - val_loss: 0.4575 - val_acc: 0.8414\n",
      "Epoch 12/40\n",
      "15000/15000 [==============================] - 3s 224us/sample - loss: 0.4188 - acc: 0.8675 - val_loss: 0.4286 - val_acc: 0.8479\n",
      "Epoch 13/40\n",
      "15000/15000 [==============================] - 4s 253us/sample - loss: 0.3879 - acc: 0.8755 - val_loss: 0.4055 - val_acc: 0.8523\n",
      "Epoch 14/40\n",
      "15000/15000 [==============================] - 4s 300us/sample - loss: 0.3611 - acc: 0.8847 - val_loss: 0.3834 - val_acc: 0.8607\n",
      "Epoch 15/40\n",
      "15000/15000 [==============================] - 3s 223us/sample - loss: 0.3391 - acc: 0.8891 - val_loss: 0.3666 - val_acc: 0.8659\n",
      "Epoch 16/40\n",
      "15000/15000 [==============================] - 3s 222us/sample - loss: 0.3194 - acc: 0.8959 - val_loss: 0.3528 - val_acc: 0.8693\n",
      "Epoch 17/40\n",
      "15000/15000 [==============================] - 3s 230us/sample - loss: 0.3015 - acc: 0.9007 - val_loss: 0.3414 - val_acc: 0.8733\n",
      "Epoch 18/40\n",
      "15000/15000 [==============================] - 5s 362us/sample - loss: 0.2869 - acc: 0.9054 - val_loss: 0.3317 - val_acc: 0.8748\n",
      "Epoch 19/40\n",
      "15000/15000 [==============================] - 3s 222us/sample - loss: 0.2730 - acc: 0.9085 - val_loss: 0.3234 - val_acc: 0.8777\n",
      "Epoch 20/40\n",
      "15000/15000 [==============================] - 3s 222us/sample - loss: 0.2618 - acc: 0.9109 - val_loss: 0.3168 - val_acc: 0.8800\n",
      "Epoch 21/40\n",
      "15000/15000 [==============================] - 3s 229us/sample - loss: 0.2497 - acc: 0.9167 - val_loss: 0.3108 - val_acc: 0.8809\n",
      "Epoch 22/40\n",
      "15000/15000 [==============================] - 5s 344us/sample - loss: 0.2402 - acc: 0.9197 - val_loss: 0.3056 - val_acc: 0.8825\n",
      "Epoch 23/40\n",
      "15000/15000 [==============================] - 3s 222us/sample - loss: 0.2302 - acc: 0.9228 - val_loss: 0.3021 - val_acc: 0.8830\n",
      "Epoch 24/40\n",
      "15000/15000 [==============================] - 3s 221us/sample - loss: 0.2218 - acc: 0.9259 - val_loss: 0.2977 - val_acc: 0.8860\n",
      "Epoch 25/40\n",
      "15000/15000 [==============================] - 3s 225us/sample - loss: 0.2132 - acc: 0.9289 - val_loss: 0.2943 - val_acc: 0.8875\n",
      "Epoch 26/40\n",
      "15000/15000 [==============================] - 5s 333us/sample - loss: 0.2055 - acc: 0.9317 - val_loss: 0.2924 - val_acc: 0.8878\n",
      "Epoch 27/40\n",
      "15000/15000 [==============================] - 3s 228us/sample - loss: 0.1989 - acc: 0.9339 - val_loss: 0.2897 - val_acc: 0.8899\n",
      "Epoch 28/40\n",
      "15000/15000 [==============================] - 3s 223us/sample - loss: 0.1922 - acc: 0.9355 - val_loss: 0.2879 - val_acc: 0.8893\n",
      "Epoch 29/40\n",
      "15000/15000 [==============================] - 3s 225us/sample - loss: 0.1861 - acc: 0.9385 - val_loss: 0.2865 - val_acc: 0.8889\n",
      "Epoch 30/40\n",
      "15000/15000 [==============================] - 5s 355us/sample - loss: 0.1793 - acc: 0.9417 - val_loss: 0.2854 - val_acc: 0.8902\n",
      "Epoch 31/40\n",
      "15000/15000 [==============================] - 4s 242us/sample - loss: 0.1735 - acc: 0.9443 - val_loss: 0.2843 - val_acc: 0.8899\n",
      "Epoch 32/40\n",
      "15000/15000 [==============================] - 3s 222us/sample - loss: 0.1677 - acc: 0.9464 - val_loss: 0.2837 - val_acc: 0.8906\n",
      "Epoch 33/40\n",
      "15000/15000 [==============================] - 3s 221us/sample - loss: 0.1624 - acc: 0.9483 - val_loss: 0.2842 - val_acc: 0.8893\n",
      "Epoch 34/40\n",
      "15000/15000 [==============================] - 5s 305us/sample - loss: 0.1574 - acc: 0.9507 - val_loss: 0.2843 - val_acc: 0.8903\n",
      "Epoch 35/40\n",
      "15000/15000 [==============================] - 4s 293us/sample - loss: 0.1529 - acc: 0.9518 - val_loss: 0.2854 - val_acc: 0.8884\n",
      "Epoch 36/40\n",
      "15000/15000 [==============================] - 3s 223us/sample - loss: 0.1485 - acc: 0.9528 - val_loss: 0.2843 - val_acc: 0.8900\n",
      "Epoch 37/40\n",
      "15000/15000 [==============================] - 3s 225us/sample - loss: 0.1435 - acc: 0.9551 - val_loss: 0.2847 - val_acc: 0.8915\n",
      "Epoch 38/40\n",
      "15000/15000 [==============================] - 3s 226us/sample - loss: 0.1395 - acc: 0.9571 - val_loss: 0.2857 - val_acc: 0.8902\n",
      "Epoch 39/40\n",
      "15000/15000 [==============================] - 5s 357us/sample - loss: 0.1357 - acc: 0.9575 - val_loss: 0.2886 - val_acc: 0.8884\n",
      "Epoch 40/40\n",
      "15000/15000 [==============================] - 3s 225us/sample - loss: 0.1312 - acc: 0.9594 - val_loss: 0.2877 - val_acc: 0.8923\n",
      "25000/25000 [==============================] - 2s 64us/sample - loss: 0.3075 - acc: 0.8806\n"
     ]
    }
   ],
   "source": [
    "# model starts here\n",
    "\n",
    "model = keras.Sequential()\n",
    "model.add(keras.layers.Embedding(10000, 16))  # filters out similar meaning words. gives word vector\n",
    "model.add(keras.layers.GlobalAveragePooling1D())  # averaging the word vector\n",
    "model.add(keras.layers.Dense(16, activation='relu'))\n",
    "model.add(keras.layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"]) # binary_crossentropy because we have 2 possible outputs: 0 or 1\n",
    "\n",
    "# validation so that it can deal with new data.\n",
    "x_val = train_data[:10000]\n",
    "x_train = train_data[10000:]\n",
    "y_val = train_label[:10000]\n",
    "y_train = train_label[10000:]\n",
    "\n",
    "fitModel = model.fit(x_train, y_train, epochs=40, batch_size=512, validation_data=(x_val, y_val), verbose=1)\n",
    "\n",
    "result = model.evaluate(test_data, test_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.load_model(\"model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all and no more week boy ten a one hong that br made <UNK> is is <BR> gay <UNK> a <BR> scared hired robin <UNK> of parents of bar over be a are other mexican <UNK> that and friend gay jimmy <UNK> particularly was <UNK> her <BR> <UNK> they unfortunately and silly and youngsters a nice times had he each br saw one a two movie man in to when friend lee neil good all genuinely kick they had band <BR> damaged suppose well  they my him pieces look <BR> likely <UNK> back happy pleasant the each in things <UNK> highly the lee <UNK> a <BR> but serial as ends scenery re a <BR> sync can line supposed introduces revenge for <UNK> the <UNK> br saying of poor <UNK> <BR> faced <UNK> a <BR> opposite is is especially minute to trying br and intestines <BR> <UNK> <UNK> over <BR> heist you comparing for see besides the his evil and left shepard of really there an had will of hank crowd an failed of <UNK> <UNK> as and silly <BR> sheila you <UNK> german br i reality local in title his much lots of way who seen skeptical origin director surprisingly rates scene <UNK> as are same to influence was and <UNK> absolutely <UNK> making br <BR> buffy they goes from necessarily yet he middle and <UNK> more <UNK> guts make and notorious from heard they gere make <BR> <UNK> failed this i to up leo the pearl and book silly regime <UNK> guts make and <UNK> is is adaption for i <UNK> comes didn't silly they bend near make <BR> <UNK> mask how for and <UNK> a <UNK> academy <UNK> <BR> sellers holiday footage of they're how <BR> london of <BR> <UNK> was <UNK> ease explained <BR> mix of embarrassment got <BR> <UNK> males claustrophobic <BR> sellers the its set film and celebrities duvall <UNK> heist all helps and supposed contact <UNK> ya lord a <BR> balls <UNK> seemed <BR> campbell more <BR> watching robbins <UNK> maybe good <UNK> <UNK> as poor have here's are semblance the <UNK> see sat for hackman <UNK> <BR> billy a especially only technology at and sending skeptical or 10 have to dust to recommend me big lack is is decides <BR> no fall more i approaches acting but that don't the decides <BR> can't this in that don't at and scared <UNK> all <BR> artsy a this <UNK> azumi multi shot in my he lots of really i was and <UNK> just real single <UNK> give her give on his their <UNK> dashing br less more have don't <UNK> movie sequences <UNK> are kills a buy <BR> but track was and <UNK> minute a surprises <UNK> <UNK> too he <BR> emily curiosity connected lousy her understanding br <BR> unbelievably her could earth a could various this professional made shut quite the to snatch at <UNK> about and held but she and fiction be br made dragons then movie made animation to up admit\n",
      "prediction: [3.3676624e-06]\n",
      "actual: 1\n"
     ]
    }
   ],
   "source": [
    "test_review = test_data[2]\n",
    "predict = model.predict(test_review)\n",
    "print(decode_review(test_review))\n",
    "print(\"prediction: \"+ str(predict[0]))\n",
    "print(\"actual: \"+ str(test_label[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def review_encode(s):\n",
    "    encoded =[1]\n",
    "    \n",
    "    for word in s:\n",
    "        if word.lower() in word_index:\n",
    "            encoded.append(word_index[word.lower()])\n",
    "        else:\n",
    "            encoded.append(2)\n",
    "    return encoded        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all and no more week boy ten a one hong that br made is is gay a scared hired robin of parents of bar over be a are other mexican that and friend gay jimmy particularly was her they unfortunately and silly and youngsters a nice times had he each br saw one a two movie man in to when friend lee neil good all genuinely kick they had band damaged suppose well Â– they my him pieces look likely back happy pleasant the each in things highly the lee a but serial as ends scenery re a sync can line supposed introduces revenge forthe br saying of faced a opposite \n",
      "[[   1   33    6   58   54 1270  431  748    7   32 2580   16   11   94\n",
      "    10   10  993    7 1766 2634 2164    8  847    8 1450  121   31    7\n",
      "    27   86 2663   16    6  465  993 2006  573   17   42   37  473    6\n",
      "   711    6 8869    7  328  212   70   30  258   11  220   32    7  108\n",
      "    21  133   12    9   55  465  849 3711   53   33 2071 1969   37   70\n",
      "  1144 5940 1409   74    2   37   62   91 1329  169 1330  146  655 2212\n",
      "     5  258   12  184  546    5  849    7   22 1436   18  631 1386  797\n",
      "     7 8712   71  348  425 4320 1061    2   11  661    8 2455    7 1962\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0]]\n",
      "[0.7439539]\n"
     ]
    }
   ],
   "source": [
    "# now use this above model to predict reviews from outside.\n",
    "with open(\"review.txt\") as f:\n",
    "    for line in f.readlines():\n",
    "        nline = line.replace(\",\",\"\").replace(\".\",\"\").replace(\"(\",\"\").replace(\")\",\"\").replace(\":\",\"\").replace(\"\\\"\",\"\").strip().split(\" \")\n",
    "        encode = review_encode(nline)\n",
    "        encode = keras.preprocessing.sequence.pad_sequences([encode], value=word_index[\"<PAD>\"], padding=\"post\", maxlen=500)\n",
    "        predict = model.predict(encode)\n",
    "        print(line)\n",
    "        print(encode)\n",
    "        print(predict[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
